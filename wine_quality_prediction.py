# -*- coding: utf-8 -*-
"""MA22AI546_Q1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UukDyCoD7CsHlpOnYNptO6jiKyYpa_f6

**Importing the libraries**
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as mtp  
import pandas as pd 
import seaborn as sns 
from google.colab import files
import io
# %matplotlib inline
from sklearn.model_selection import train_test_split  
from sklearn.preprocessing import StandardScaler

"""**Uploading the file from local system**"""

data = files.upload()

"""**Loading the dataset**"""

wine_data = pd.read_csv(io.StringIO(data['wine.csv'].decode('utf-8')))

wine_data.head(5)

"""**Checking the shape of the dataset**"""

wine_data.shape

"""**Checking the column name**"""

wine_data.columns

"""**Check the data types of data**"""

wine_data.info()

"""**Finding the missing value**"""

wine_data.isnull().sum()

wine_data.describe().transpose()

sns.pairplot(wine_data)

"""**Changing Categorical value into numerical value for qualiy column**"""

wine_data['quality'].replace(['bad', 'good'],[0, 1], inplace=True)
wine_data.head()

"""Above changes the categorical value into numerical value as for bad quality value is "0" and for good quality value is "1"

**Feature Selection**
"""

x = wine_data.iloc[:,0:11].values  
y = wine_data.iloc[:,11].values

x

y

"""**Spliting the dataset into training set and testing set**"""

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.30, random_state = 42)

"""**Feature Scaling**"""

sc = StandardScaler()  
x_train = sc.fit_transform(x_train)  
x_test = sc.transform(x_test)

"""**Fitting Naive Bayes to the Training Set**

After the pre-processing step, now we will fit the Naive Bayes model to the Training set
"""

from sklearn.naive_bayes import GaussianNB  
model = GaussianNB()  
model.fit(x_train, y_train)

"""**Prediction of the test set result**"""

y_pred = model.predict(x_test)

y_pred

"""**Cross Validation Score**"""

from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score
cv_nb = cross_val_score(estimator = model, X = x_train, y = y_train.ravel(), cv = 10)
print("CV: ", cv_nb.mean())

y_pred_train = model.predict(x_train)
accuracy_train = accuracy_score(y_train, y_pred_train)
print("Training set: ", accuracy_train)

y_pred_test = model.predict(x_test)
accuracy_test = accuracy_score(y_test, y_pred_test)
print("Test set: ", accuracy_test)

model.score(x_test,y_test)

"""**Confusion Matrix**"""

from sklearn.metrics import confusion_matrix  
confusion_matrix(y_test, y_pred_test)

from sklearn import metrics
report = metrics.classification_report(y_test, y_pred_test, output_dict=True)
df_classification_report = pd.DataFrame(report).transpose()
df_classification_report = df_classification_report.sort_values(by=['f1-score'], ascending=False)
df_classification_report

tp_nb = confusion_matrix(y_test, y_pred_test)[0,0]
fp_nb = confusion_matrix(y_test, y_pred_test)[0,1]
tn_nb = confusion_matrix(y_test, y_pred_test)[1,1]
fn_nb = confusion_matrix(y_test, y_pred_test)[1,0]

"""**Logistic Regression**"""

# Fitting Logistic Regression to the Training set
from sklearn.linear_model import LogisticRegression
model_lr = LogisticRegression(C=1, fit_intercept=True, max_iter=1000, penalty = 'l2', solver='liblinear')
model_lr.fit(x_train, y_train.ravel())

"""**Predicting cross validation for logistic regression**"""

cv_lr = cross_val_score(estimator = model_lr, X = x_train, y = y_train.ravel(), cv = 10)
print("CV: ", cv_lr.mean())

y_pred_lr_train = model_lr.predict(x_train)
accuracy_lr_train = accuracy_score(y_train, y_pred_lr_train)
print("Training set: ", accuracy_lr_train)

y_pred_lr_test = model_lr.predict(x_test)
accuracy_lr_test = accuracy_score(y_test, y_pred_lr_test)
print("Test set: ", accuracy_lr_test)

"""**Confusion Matrix for logistic Regression**"""

confusion_matrix(y_test, y_pred_lr_test)

lr_report = metrics.classification_report(y_test, y_pred_lr_test, output_dict=True)
lr_classification_report = pd.DataFrame(lr_report).transpose()
lr_classification_report = lr_classification_report.sort_values(by=['f1-score'], ascending=False)
lr_classification_report

tp_lr = confusion_matrix(y_test, y_pred_lr_test)[0,0]
fp_lr = confusion_matrix(y_test, y_pred_lr_test)[0,1]
tn_lr = confusion_matrix(y_test, y_pred_lr_test)[1,1]
fn_lr = confusion_matrix(y_test, y_pred_lr_test)[1,0]

"""**ROC Curve**"""

classes_q = sorted(wine_data.quality.unique())
classes_q

from sklearn.metrics import roc_curve

fpr, tpr, thresholds = roc_curve(y_test, y_pred)

mtp.figure(figsize=(6,4))

mtp.plot(fpr, tpr, linewidth=2)

mtp.plot([0,1], [0,1], 'k--' )

mtp.rcParams['font.size'] = 12

mtp.title('ROC curve for Wine quality')

mtp.xlabel('False Positive Rate (1 - Specificity)')

mtp.ylabel('True Positive Rate (Sensitivity)')

mtp.show()

models = [('Naive Bayes', tp_nb, fp_nb, tn_nb, fn_nb, accuracy_train, accuracy_test, cv_nb.mean()),('Logistic Regression', tp_lr, fp_lr, tn_lr, fn_lr, accuracy_lr_train, accuracy_lr_test, cv_lr.mean())]

output = pd.DataFrame(data = models, columns=['Model', 'True Positive', 'False Positive', 'True Negative',
                                               'False Negative', 'Accuracy(training)', 'Accuracy(test)',
                                               'Cross-Validation'])
output

